{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aWfhkEbq-RO"
      },
      "source": [
        "# Implementing a Tiny GPT Language Model in Jax\n",
        "\n",
        "In this tutorial, we'll guide you through building a simplified version of the GPT (Generative Pre-trained Transformer) model using Jax. This compact model will serve as a practical introduction to the world of language models and the Jax framework. While inspired by the original GPT-2 architecture presented in the GPT-2 paper, our model will be significantly smaller to ensure a manageable and efficient learning experience.\n",
        "\n",
        "\n",
        "You can find more details about GPT models here: [GPT](https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035), [GPT-2](https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe),[GPT-3](https://arxiv.org/abs/2005.14165).\n",
        "We also used [nanoGPT](https://github.com/karpathy/nanoGPT) as an inspirational example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Kb_Lk8_uS8x"
      },
      "source": [
        "#1. Setup and Imports\n",
        "\n",
        "First, let's install the necessary libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4uo3iu-vy_X",
        "outputId": "c8778f3b-8983-429e-c478-a4572d427d26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: regex\u003e=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests\u003e=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests\u003e=2.26.0-\u003etiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests\u003e=2.26.0-\u003etiktoken) (3.8)\n",
            "Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests\u003e=2.26.0-\u003etiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests\u003e=2.26.0-\u003etiktoken) (2024.7.4)\n"
          ]
        }
      ],
      "source": [
        "# !pip install git+https://github.com/google/flax.\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lsf3XXIwudwv"
      },
      "source": [
        "And import the necessary libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bW84xMouwWr-"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "import functools\n",
        "from typing import Any\n",
        "from flax import nnx\n",
        "from flax.nnx.nnx.nn import initializers\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkzu6jCFupWU"
      },
      "source": [
        "#2. Model Architecture\n",
        "\n",
        "We'll define the fundamental building blocks of the GPT-2 model:\n",
        "* SelfAttention\n",
        "* MlpBlock\n",
        "* CausalAttentionBlock\n",
        "\n",
        "\n",
        "Then stack these blocks to form our tiny GPT model:\n",
        "* GPTModel\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNaiVJIrAGdx"
      },
      "source": [
        "##2.0 Model Config\n",
        "\n",
        "Before we start with building blocks, let's define config , so rest of the component can use common config."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RACulI5AQJ8"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "  \"\"\"Configuration for a demo language model.\"\"\"\n",
        "\n",
        "  # The dimensionality of the embeddings (i.e., the size of the vector representing each word/token).\n",
        "  num_embd: int\n",
        "\n",
        "  # The number of attention heads in each multi-head attention layer.\n",
        "  num_heads: int\n",
        "\n",
        "  # The number of transformer blocks (layers) in the model.\n",
        "  num_layers: int\n",
        "\n",
        "  # The size of the vocabulary (i.e., the number of unique words/tokens the model knows).\n",
        "  vocab_size: int\n",
        "\n",
        "  # The maximum context length (i.e., the number of previous tokens the model considers when predicting the next one).\n",
        "  block_size: int\n",
        "\n",
        "  # The dropout probability for the attention layer.\n",
        "  attn_pdrop: float\n",
        "\n",
        "  # The dropout probability for the residual connections.\n",
        "  resid_pdrop: float\n",
        "\n",
        "  # The dropout probability for the embedding layer.\n",
        "  embd_pdrop: float"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AER9I-2Szw7t"
      },
      "source": [
        "##2.1 Attention layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dc7hSXmowAhb"
      },
      "outputs": [],
      "source": [
        "class SelfAttention(nnx.Module):\n",
        "  \"\"\"Implements a causal self-attention layer with a projecttion at the end.\n",
        "\n",
        "  It is possible to use nnx.MultiHeadAttention instead.\n",
        "  \"\"\"\n",
        "\n",
        "  # Configurable parameters\n",
        "  num_heads: int\n",
        "  head_dim: int\n",
        "\n",
        "  def __init__(self, config: Config, rngs: nnx.Rngs):\n",
        "    super().__init__()\n",
        "\n",
        "    self.num_heads = config.num_embd\n",
        "    self.head_dim = config.num_embd // config.num_heads\n",
        "\n",
        "    # key, query, value projections for all heads.\n",
        "    out_features = (config.num_heads, self.head_dim)\n",
        "    self.q_attn = nnx.LinearGeneral(\n",
        "        in_features=config.num_embd, out_features=out_features, rngs=rngs\n",
        "    )\n",
        "    self.k_attn = nnx.LinearGeneral(\n",
        "        in_features=config.num_embd, out_features=out_features, rngs=rngs\n",
        "    )\n",
        "    self.v_attn = nnx.LinearGeneral(\n",
        "        in_features=config.num_embd, out_features=out_features, rngs=rngs\n",
        "    )\n",
        "\n",
        "    # output projection\n",
        "    self.out = nnx.LinearGeneral(\n",
        "        in_features=out_features,\n",
        "        out_features=config.num_embd,\n",
        "        axis=(-2, -1),\n",
        "        rngs=rngs,\n",
        "    )\n",
        "\n",
        "    # regularization\n",
        "    self.attn_dropout = nnx.Dropout(config.attn_pdrop, rngs=rngs)\n",
        "    self.resid_dropout = nnx.Dropout(config.resid_pdrop, rngs=rngs)\n",
        "\n",
        "  def __call__(self, x, mask, train: bool = True):\n",
        "    # batch, sequence length, embedding dimensionality\n",
        "    B, T, C = x.shape\n",
        "    dtype = x.dtype\n",
        "\n",
        "    # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "    k = self.k_attn(x)\n",
        "    q = self.q_attn(x)\n",
        "    v = self.v_attn(x)\n",
        "\n",
        "    # Scaled dot-product attention\n",
        "    q = q / jnp.sqrt(self.head_dim).astype(dtype)\n",
        "    # attn weight shape is (batch..., num_heads, q_length, kv_length)\n",
        "    attn_weights = jnp.einsum('...qhd,...khd-\u003e...hqk', q, k)\n",
        "\n",
        "    # apply mask\n",
        "    big_neg = jnp.finfo(dtype).min\n",
        "    attn_weights = jnp.where(mask, attn_weights, big_neg)\n",
        "\n",
        "    # normalize the attention weights\n",
        "    attn_weights = nnx.softmax(attn_weights, axis=-1)\n",
        "    # apply the dropout mask\n",
        "    attn_weights = self.attn_dropout(attn_weights, deterministic=not train)\n",
        "\n",
        "    # Attention output\n",
        "    y = jnp.einsum('...hqk,...khd-\u003e...qhd', attn_weights, v)\n",
        "    # reshape back to batch, sequence length, embedding dimensionality and apply dropout\n",
        "    y = self.resid_dropout(self.out(y), deterministic=not train)\n",
        "\n",
        "    return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivJVbH9Kzu0r"
      },
      "source": [
        "##2.2 Feed-Forward Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAQgWnnTz3G4"
      },
      "outputs": [],
      "source": [
        "class MlpBlock(nnx.Module):\n",
        "\n",
        "  def __init__(self, config: Config, rngs: nnx.Rngs):\n",
        "    super().__init__()\n",
        "    self.c_fc = nnx.Linear(config.num_embd, 4 * config.num_embd, rngs=rngs)\n",
        "    self.c_proj = nnx.Linear(4 * config.num_embd, config.num_embd, rngs=rngs)\n",
        "    self.dropout = nnx.Dropout(config.resid_pdrop, rngs=rngs)\n",
        "    self.act = nnx.gelu\n",
        "\n",
        "  def __call__(self, x, train: bool = True) -\u003e Any:\n",
        "    x = self.c_fc(x)\n",
        "    x = self.act(x)\n",
        "    x = self.dropout(x, deterministic=not train)\n",
        "    x = self.c_proj(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQyPNk290MWr"
      },
      "source": [
        "##2.3 Transformer Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1e8PIcu80PHz"
      },
      "outputs": [],
      "source": [
        "class AttentionBlock(nnx.Module):\n",
        "\n",
        "  def __init__(self, config: Config, rngs: nnx.Rngs):\n",
        "    super().__init__()\n",
        "    self.ln_1 = nnx.LayerNorm(num_features=config.num_embd, rngs=rngs)\n",
        "    self.attn = SelfAttention(config, rngs=rngs)\n",
        "    self.ln_2 = nnx.LayerNorm(num_features=config.num_embd, rngs=rngs)\n",
        "    self.mlp = MlpBlock(config, rngs=rngs)\n",
        "\n",
        "  def __call__(self, x, mask, train: bool = True):\n",
        "    ln_x = self.ln_1(x)\n",
        "    attn_x = self.attn(ln_x, mask, train)\n",
        "    x = x + attn_x\n",
        "\n",
        "    ln2_x = self.ln_2(x)\n",
        "    mlp_x = self.mlp(ln2_x, train)\n",
        "    x = x + mlp_x\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVmd0W6IAmFS"
      },
      "source": [
        "##2.4 GPTModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjSA8zDVAqOo"
      },
      "outputs": [],
      "source": [
        "def causal_attention_mask(batch_size, n_dest, n_src, dtype=jnp.float32):\n",
        "  \"\"\"Auxilary function to create a causal attention mask.\n",
        "\n",
        "  Mask the upper half of the dot product matrix in self attention.\n",
        "  This prevents flow of information from future tokens to current token.\n",
        "  1's in the lower triangle, counting from the lower right corner.\n",
        "  \"\"\"\n",
        "  # [B, 1, SRC, DST]\n",
        "  return jnp.tril(jnp.ones((batch_size, 1, n_dest, n_src), dtype=dtype))\n",
        "\n",
        "\n",
        "class GPTModel(nnx.Module):\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "    rngs = nnx.Rngs(0)\n",
        "    self.wte = nnx.Embed(config.vocab_size, config.num_embd, rngs=rngs)\n",
        "    self.wpe = nnx.Embed(config.block_size, config.num_embd, rngs=rngs)\n",
        "    self.dropout = nnx.Dropout(config.embd_pdrop, rngs=rngs)\n",
        "    # list of attention blocks\n",
        "    self.h = [\n",
        "        AttentionBlock(config, rngs=rngs) for _ in range(config.num_layers)\n",
        "    ]\n",
        "    # layer norm before output logits\n",
        "    self.ln_f = nnx.LayerNorm(num_features=config.num_embd, rngs=rngs)\n",
        "    # predicted logits\n",
        "    self.ln_logits = nnx.Linear(config.num_embd, config.vocab_size, rngs=rngs)\n",
        "    # keep reference on config\n",
        "    self.config = config\n",
        "\n",
        "  def __call__(self, idx, targets=None, train: bool = True):\n",
        "    # batch, sequence length\n",
        "    B, T = idx.shape\n",
        "    tok_emb = self.wte(idx)  # (B,T,C)\n",
        "    pos_emb = self.wpe(jnp.arange(T))  # (T,C)\n",
        "    x = self.dropout(tok_emb + pos_emb, deterministic=not train)\n",
        "    casual_mask = causal_attention_mask(B, T, T, dtype=jnp.float32)\n",
        "\n",
        "    for block in self.h:\n",
        "      x = block(x, casual_mask, train)\n",
        "\n",
        "    x = self.ln_f(x)\n",
        "    logits = self.ln_logits(x)\n",
        "\n",
        "    # if we are given some desired targets also calculate the loss\n",
        "    loss = None\n",
        "    if targets is not None:\n",
        "      loss = optax.losses.softmax_cross_entropy_with_integer_labels(\n",
        "          logits=logits, labels=targets\n",
        "      ).mean()\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, prompt, max_length=100, temperature=1.0, top_k=0):\n",
        "    input_ids = jnp.array(prompt, dtype=jnp.int32)[None, ...]\n",
        "    # pad idx up to block_size\n",
        "    idx_size = input_ids.shape[1]\n",
        "    pad_len = self.config.block_size - idx_size\n",
        "    if pad_len \u003e 0:\n",
        "      input_ids = jnp.concatenate(\n",
        "          (input_ids, jnp.zeros((1, pad_len), dtype=jnp.int32)), axis=1\n",
        "      )\n",
        "    # idx is (B, T) array of indices in the current context\n",
        "    predict_idx = min(idx_size - 1, self.config.block_size - 1)\n",
        "    for _ in range(max_length):\n",
        "      # crop idx to the last block_size tokens\n",
        "      idx_cond = input_ids[:, -self.config.block_size :]\n",
        "      # get the predictions\n",
        "      logits, loss = self(idx_cond, train=False)\n",
        "      # focus only on the last time step\n",
        "      logits = logits[0, predict_idx, :] / temperature\n",
        "      if top_k \u003e 0:\n",
        "        top_k_logits, top_k_indices = jax.lax.top_k(logits, top_k)\n",
        "        probs = nnx.softmax(top_k_logits)\n",
        "        next_token = jax.random.categorical(jax.random.PRNGKey(0), probs)\n",
        "        next_token = top_k_indices[next_token]\n",
        "      else:\n",
        "        probs = nnx.softmax(logits)\n",
        "        next_token = jax.random.categorical(jax.random.PRNGKey(0), logits)\n",
        "\n",
        "      if (\n",
        "          next_token == self.config.vocab_size - 1\n",
        "          or next_token == tiktoken.get_encoding(\"gpt2\").eot_token\n",
        "      ):\n",
        "        break\n",
        "      # append sampled index to the running sequence\n",
        "      if predict_idx == self.config.block_size - 1:\n",
        "        input_ids = jnp.concatenate((input_ids, next_token[None, None]), axis=1)\n",
        "      else:\n",
        "        input_ids = input_ids.at[0, predict_idx + 1].set(next_token)\n",
        "      predict_idx = min(predict_idx + 1, self.config.block_size - 1)\n",
        "\n",
        "    return input_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAC7OczuMpQa"
      },
      "source": [
        "#3. Initializing and Training\n",
        "\n",
        "Let's initialize our GPT model, set up the training state, and define the loss function and training step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWWynughNCVG"
      },
      "source": [
        "# 3. Dataset\n",
        "\n",
        "Now, let's prepare data. In this tutorial, we'll be using:\n",
        "1. tiktoken tokenizer to process our text data. Tiktoken is a fast byte-pair encoding (BPE) tokenizer designed for use with OpenAI models. While we've chosen tiktoken for its efficiency, it's worth noting that other tokenizers, such as SentencePiece, could also be used depending on your specific requirements.\n",
        "2. [tinyshakespeare](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt) data as a toy dataset.\n",
        "\n",
        "## Preparing Train and Eval Datasets\n",
        "\n",
        "Before feeding our data to the model, we'll divide it into two sets: a training set and an evaluation set.\n",
        "\n",
        "**Key Point**: Transformer models expect numerical input, so tokenization is a crucial preprocessing step. It's the bridge between human-readable text and the numerical representations that the model operates on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDqX7FQEMo5z",
        "outputId": "e85d2645-a6c0-4191-db16-6d4279aaa965"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train has 301,966 tokens\n",
            "val has 36,059 tokens\n"
          ]
        }
      ],
      "source": [
        "# Original source: https://github.com/karpathy/nanoGPT/blob/master/data/shakespeare/prepare.py\n",
        "import os\n",
        "import numpy as np\n",
        "import requests\n",
        "import tiktoken\n",
        "\n",
        "# download the tiny shakespeare dataset\n",
        "input_file_dir = '/tmp'\n",
        "input_file_path = os.path.join(input_file_dir, 'input.txt')\n",
        "if not os.path.exists(input_file_path):\n",
        "  data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "  with open(input_file_path, 'w', encoding='utf-8') as f:\n",
        "    f.write(requests.get(data_url).text)\n",
        "\n",
        "with open(input_file_path, 'r', encoding='utf-8') as f:\n",
        "  data = f.read()\n",
        "n = len(data)\n",
        "train_data = data[: int(n * 0.9)]\n",
        "val_data = data[int(n * 0.9) :]\n",
        "\n",
        "# encode with tiktoken gpt2 bpe\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "train_ids = enc.encode_ordinary(train_data)\n",
        "val_ids = enc.encode_ordinary(val_data)\n",
        "print(f'train has {len(train_ids):,} tokens')\n",
        "print(f'val has {len(val_ids):,} tokens')\n",
        "\n",
        "# export to bin files\n",
        "train_ids = np.array(train_ids, dtype=np.uint16)\n",
        "val_ids = np.array(val_ids, dtype=np.uint16)\n",
        "train_ids.tofile(os.path.join(input_file_dir, 'train.bin'))\n",
        "val_ids.tofile(os.path.join(input_file_dir, 'val.bin'))\n",
        "\n",
        "# train.bin has 301,966 tokens\n",
        "# val.bin has 36,059 tokens\n",
        "\n",
        "\n",
        "def get_batch(split, data_dir, block_size, batch_size):\n",
        "  # generate a small batch of data of inputs x and targets y\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  # We recreate np.memmap every batch to avoid a memory leak, as per\n",
        "  # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
        "  if split == 'train':\n",
        "    data = np.memmap(\n",
        "        os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r'\n",
        "    )\n",
        "  else:\n",
        "    data = np.memmap(\n",
        "        os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r'\n",
        "    )\n",
        "  ix = np.random.randint(0, len(data) - block_size, (batch_size,))\n",
        "  x = jnp.stack(\n",
        "      [jnp.asarray((data[i : i + block_size]).astype(np.int64)) for i in ix]\n",
        "  )\n",
        "  y = jnp.stack([\n",
        "      jnp.asarray((data[i + 1 : i + 1 + block_size]).astype(np.int64))\n",
        "      for i in ix\n",
        "  ])\n",
        "\n",
        "  return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Wz2VP8jOyd6"
      },
      "source": [
        "#4. Initializing and Training\n",
        "\n",
        "Let's initialize our GPT-2 model, define the loss function and training step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8exPVK3MhtE"
      },
      "outputs": [],
      "source": [
        "# define config, using reduced number of heads and layers to speed up tutorial.\n",
        "config = Config(\n",
        "    num_embd=768,\n",
        "    num_heads=8,\n",
        "    num_layers=8,\n",
        "    vocab_size=tiktoken.get_encoding(\"gpt2\").n_vocab,\n",
        "    block_size=256,\n",
        "    attn_pdrop=0.1,\n",
        "    resid_pdrop=0.1,\n",
        "    embd_pdrop=0.1,\n",
        ")\n",
        "\n",
        "model = GPTModel(config)\n",
        "\n",
        "\n",
        "def loss_fn(model, x, y):\n",
        "  logits, loss = model(x, y)  # call methods directly\n",
        "  return loss, logits\n",
        "\n",
        "\n",
        "@nnx.jit  # automatic state management\n",
        "def train_step(model, optimizer, x, y, metrics):\n",
        "  (loss, logits), grads = nnx.value_and_grad(loss_fn, has_aux=True)(model, x, y)\n",
        "  optimizer.update(grads)  # inplace updates\n",
        "  metrics.update(loss=loss, logits=logits, labels=y)\n",
        "  return loss, logits\n",
        "\n",
        "\n",
        "@nnx.jit\n",
        "def eval_step(model, x, y, metrics):\n",
        "  loss, logits = loss_fn(x, y)  # call methods directly\n",
        "  metrics.update(loss=loss, logits=logits, labels=y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnADgBL6P2T5"
      },
      "source": [
        "Now, define optimizer, define training loop and start training the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFhJNKojP_ug",
        "outputId": "e40eb964-e096-4fbf-cbd3-dac68da8b6ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[train] epoch[0]: accuracy: 0.000\n",
            "[train] epoch[0]: loss: 11.221\n",
            "[train] epoch[1000]: accuracy: 0.249\n",
            "[train] epoch[1000]: loss: 4.808\n",
            "[train] epoch[2000]: accuracy: 0.304\n",
            "[train] epoch[2000]: loss: 4.110\n",
            "[train] epoch[3000]: accuracy: 0.381\n",
            "[train] epoch[3000]: loss: 3.456\n",
            "[train] epoch[4000]: accuracy: 0.483\n",
            "[train] epoch[4000]: loss: 2.825\n",
            "[train] epoch[5000]: accuracy: 0.567\n",
            "[train] epoch[5000]: loss: 2.348\n",
            "[train] epoch[6000]: accuracy: 0.629\n",
            "[train] epoch[6000]: loss: 2.005\n",
            "[train] epoch[7000]: accuracy: 0.675\n",
            "[train] epoch[7000]: loss: 1.750\n",
            "[train] epoch[8000]: accuracy: 0.710\n",
            "[train] epoch[8000]: loss: 1.554\n",
            "[train] epoch[9000]: accuracy: 0.739\n",
            "[train] epoch[9000]: loss: 1.398\n",
            "[train] epoch[10000]: accuracy: 0.762\n",
            "[train] epoch[10000]: loss: 1.272\n",
            "[train] epoch[11000]: accuracy: 0.781\n",
            "[train] epoch[11000]: loss: 1.167\n",
            "[train] epoch[12000]: accuracy: 0.797\n",
            "[train] epoch[12000]: loss: 1.079\n",
            "[train] epoch[13000]: accuracy: 0.811\n",
            "[train] epoch[13000]: loss: 1.004\n",
            "[train] epoch[14000]: accuracy: 0.823\n",
            "[train] epoch[14000]: loss: 0.939\n",
            "[train] epoch[15000]: accuracy: 0.833\n",
            "[train] epoch[15000]: loss: 0.882\n",
            "[train] epoch[16000]: accuracy: 0.842\n",
            "[train] epoch[16000]: loss: 0.832\n",
            "[train] epoch[17000]: accuracy: 0.851\n",
            "[train] epoch[17000]: loss: 0.788\n",
            "[train] epoch[18000]: accuracy: 0.858\n",
            "[train] epoch[18000]: loss: 0.748\n",
            "[train] epoch[19000]: accuracy: 0.865\n",
            "[train] epoch[19000]: loss: 0.713\n",
            "[train] epoch[20000]: accuracy: 0.871\n",
            "[train] epoch[20000]: loss: 0.681\n",
            "[train] epoch[21000]: accuracy: 0.876\n",
            "[train] epoch[21000]: loss: 0.651\n",
            "[train] epoch[22000]: accuracy: 0.881\n",
            "[train] epoch[22000]: loss: 0.625\n",
            "[train] epoch[23000]: accuracy: 0.886\n",
            "[train] epoch[23000]: loss: 0.600\n",
            "[train] epoch[24000]: accuracy: 0.890\n",
            "[train] epoch[24000]: loss: 0.578\n"
          ]
        }
      ],
      "source": [
        "optimizer = nnx.Optimizer(model, optax.adamw(learning_rate=1e-4))\n",
        "metrics = nnx.MultiMetric(\n",
        "    accuracy=nnx.metrics.Accuracy(),\n",
        "    loss=nnx.metrics.Average('loss'),\n",
        ")\n",
        "\n",
        "# Keep results for plotting\n",
        "metrics_history = {\n",
        "    'train_loss': [],\n",
        "    'train_accuracy': [],\n",
        "}\n",
        "\n",
        "# Feel free to experiment with the number of steps.\n",
        "num_steps = 25000\n",
        "\n",
        "for step in range(num_steps):\n",
        "  # will optimize for bigger batch size by subsequent tutorials,\n",
        "  # using batch size 32 for now.\n",
        "  x, y = get_batch('train', input_file_dir, config.block_size, 32)\n",
        "\n",
        "  train_step(model, optimizer, x, y, metrics)\n",
        "  for metric, value in metrics.compute().items():  # compute metrics\n",
        "    metrics_history[f'train_{metric}'].append(value)  # record metrics\n",
        "    if step % 1000 == 0:\n",
        "      print(f'[train] step[{step}]: {metric}: {value:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhA9oHWLQdj4"
      },
      "source": [
        "# 5. Generate text\n",
        "\n",
        "Let's evaluate the capabilities of our compact model by assessing its capacity to generate text reminiscent of Shakespearean prose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0Q02AgIWYzO",
        "outputId": "7c2f204d-9341-45e4-cbfb-f5fc9b0f3337"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=========    Prefix   =======\n",
            " power, I would\n",
            "Have sunk the sea within the earth or ere\n",
            "It should the good ship so have swallow'd\n",
            "=========    Generated using top_k=1 =======\n",
            " power, I would\n",
            "Have sunk the sea within the earth or ere\n",
            "It should the good ship so have swallow'd all the kindred of the Capulets lie.\n",
            "In the mean time, against thou shalt awake,\n",
            "Shall Romeo by my letters know our drift,\n",
            "And hither shall he come: and he and I\n",
            "Will watch thy waking, and that very night\n",
            "Shall Romeo bear thee hence to Mantua.\n",
            "And this shall free thee from this present shame;\n",
            "If no inconstant toy, nor womanish fear,\n",
            "Abate thy valour in the acting it.\n",
            "\n",
            "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
            "=========    Generated not using top_k  =======\n",
            " power, I would\n",
            "Have sunk the sea within the earth or ere\n",
            "It should the good ship so have swallow'd all swords.\n",
            "\n",
            "KING RICHARD II:\n",
            "Why, uncle, what's the matter?\n",
            "\n",
            "DUKE OF YORK:\n",
            "O my liege,\n",
            "Pardon me, if you please; if not, I, pleased\n",
            "Not to be pardon'd, am content withal.\n",
            "Seek you to seize and gripe into your hands\n",
            "The royalties and rights of banish'd Hereford?\n",
            "Is not Gaunt dead, and doth not Hereford live!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
          ]
        }
      ],
      "source": [
        "x, y = get_batch(\"val\", input_file_dir, 125, 1)\n",
        "print(\"=========    Prefix   =======\")\n",
        "print(enc.decode(x[0].tolist()[:25]))\n",
        "print(\"=========    Generated using top_k=1 =======\")\n",
        "print(enc.decode(GPTModel.generate(model, x[0, :25], top_k=1)[0].tolist()))\n",
        "print(\"=========    Generated not using top_k  =======\")\n",
        "print(enc.decode(GPTModel.generate(model, x[0, :25])[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUKPDBwmRgUH"
      },
      "source": [
        "# 6. Conclusion\n",
        "\n",
        "This tutorial has provided a foundational exploration of the GPT-2 model architecture, implemented using Jax and Flax frameworks. While this implementation serves as an educational starting point, it's important to note that further development would be necessary to create a production-ready language model.\n",
        "Key Takeaways:\n",
        "\n",
        "* **Core Transformer Components:** We've successfully implemented the essential elements of a transformer architecture, including self-attention mechanisms and feed-forward networks.\n",
        "* **Positional Information:** The model incorporates learned positional embeddings, crucial for sequence understanding in transformer models.\n",
        "* **Training Loop:** A basic training loop has been established, utilizing cross-entropy loss and the Adam optimizer, demonstrating the fundamental steps in model training.\n",
        "* **Data Processing:** The tutorial showcases the use of the tiktoken tokenizer, illustrating an approach to prepare textual data for input into the language model.\n",
        "\n",
        "This implementation provides a solid foundation for further exploration and experimentation in the field of language modeling using Jax and Flax. Users are encouraged to build upon this framework, modify the architecture, and delve deeper into advanced concepts in natural language processing and transformer models."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
